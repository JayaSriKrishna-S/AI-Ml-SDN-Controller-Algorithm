{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m MinMaxScaler\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv(\"sdn_upda2.csv\",index_col='TimeStamp',parse_dates=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.drop(['IP Address','s4','Priority','s2','s3','s5','Membership'],axis=1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=MinMaxScaler(feature_range=(0,1))\n",
    "dataset=sc.fit_transform(dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_x=[]\n",
    "Train_y=[]\n",
    "for i in range(50,6048):\n",
    "   Train_x.append(dataset[i-50:i,0])\n",
    "   Train_y.append(dataset[i])\n",
    "\n",
    "Train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_x=np.array(Train_x)\n",
    "Train_y=np.array(Train_y)\n",
    "Train_x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Train_x=np.reshape(Train_x,(Train_x.shape[0],Train_x.shape[1],1))\n",
    "Train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unix=tf.keras.Sequential()\n",
    "unix.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=100, return_sequences=True,input_shape=(Train_x.shape[1],1))))\n",
    "unix.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "unix.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=100,return_sequences=True)))\n",
    "unix.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "unix.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=50)))\n",
    "unix.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "unix.add(tf.keras.layers.Dense(units=1,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unix.compile(optimizer='adam',loss='mean_squared_error',metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BILSTM=unix.fit(Train_x,Train_y,epochs=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(sc, open('../S1_upda2/unixscaler_BL5.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_jsk = unix.to_json()\n",
    "with open(\"../S1_upda2/Traffic_model_BL5.json\", \"w\") as source:\n",
    "    source.write(model_jsk)\n",
    "# save model parameter\n",
    "unix.save_weights(\"../S1_upda2/Traffic_model_BL5.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc= pickle.load(open('../S1_upda2/unixscaler_BL5.pkl','rb'))\n",
    "\n",
    "with open(\"../S1_upda2/Traffic_model_BL5.json\", \"r\") as f:\n",
    "    unix = model_from_json(f.read())\n",
    "unix.load_weights(\"../S1_upda2/Traffic_model_BL5.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv(\"sdn_upda2.csv\",index_col='TimeStamp',parse_dates=True)\n",
    "dataset=dataset.drop(['IP Address','s4','Priority','s2','s3','s5','Membership'],axis=1)\n",
    "copy=pd.read_csv(\"sdn_upda2.csv\",index_col='TimeStamp',parse_dates=True)\n",
    "copy=copy.drop(['IP Address','s4','Priority','s2','s3','s5','Membership'],axis=1)\n",
    "copy=copy.values\n",
    "\n",
    "\n",
    "\n",
    "with open(\"s1\", \"rb\") as fp:   # Unpickling\n",
    " sarima = pickle.load(fp)\n",
    "\n",
    "\n",
    "dataset=sc.fit_transform(dataset)\n",
    "real_unixred=dataset[6049:,0]\n",
    "real_unix=copy[6049:,0]\n",
    "x_test=[]\n",
    "for i in range(6048,8064):\n",
    "    x_test.append(dataset[i-50:i,0])\n",
    "\n",
    "X_test=np.array(x_test)\n",
    "\n",
    "X_test=np.reshape(X_test,(X_test.shape[0],X_test.shape[1],1))\n",
    "predicted_unixred=unix.predict(X_test)\n",
    "\n",
    "predicted_unix=sc.inverse_transform(predicted_unixred)\n",
    "print(predicted_unix)\n",
    "\n",
    "\n",
    "with open(\"bilstm\", \"wb\") as source:\n",
    "    pickle.dump(predicted_unix,source)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0.1, 0.1, 0.8, 0.8]) \n",
    "y = sarima\n",
    "x = [i for i in range(0, 288)]\n",
    "\n",
    "ax.plot(x,predicted_unix[:288],color=\"red\",label='Bilstm Prediction')\n",
    "ax.plot(x,y[:288],color=\"yellow\",label='Sarima Prediction')\n",
    "ax.plot(x,real_unix[:288],color=\"green\",label='Real')\n",
    "ax.set_xlabel('Hours in predicted Day')\n",
    "ax.set_ylabel('Traffic in Gigabytes')\n",
    "ax.set_title('Prediction Comparision')\n",
    "ax.set_xticks([i for i in range(0,289,24)])\n",
    "ax.legend()\n",
    "ax.set_xticklabels([j for j in range(0,25,2)])\n",
    "# plt.axvspan(1446, 1451, facecolor='r', alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\srija\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/srija/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "\n",
    "\n",
    "print(\"MEAN SQUARE ERROR\")\n",
    "print(np.sqrt(mean_squared_error(real_unixred,predicted_unixred[:-1])))\n",
    "print()\n",
    "print(\"MEAN ABSOLUTE ERROR\")\n",
    "print(mean_absolute_error(real_unixred,predicted_unixred[:-1]))\n",
    "\n",
    "mape = np.mean(np.abs(real_unix-predicted_unix)/real_unix)*100\n",
    "mape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"predweek2upda\", \"wb\") as fp:   #Pickling\n",
    "#    pickle.dump(prdiction_Switch, fp)\n",
    "\n",
    "\n",
    "# import pickle\n",
    "# with open(\"predweek2upda\", \"rb\") as fp:   # Unpickling\n",
    "#  jsk = pickle.load(fp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e8d894d7e940e7a2da94a5982c2e4861ed0552cc90195a4d8f88b7ffae6ca48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
